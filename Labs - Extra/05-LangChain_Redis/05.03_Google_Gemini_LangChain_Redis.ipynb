{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD93SPoaoErY"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redislabs-Solution-Architects/Redis-Workshops/blob/main/05-LangChain_Redis/05.03_Google_Gemini_LangChain_Redis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2-i8jBl9GRH"
      },
      "source": [
        "# Document Question Answering with Langchain, Google Gemini and Redis\n",
        "\n",
        "![Redis](https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120)\n",
        "\n",
        "This notebook would use your own deployment of VertexAI, Redis with Vector Similarity Search and LangChain to answer questions about the information contained in a document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZsd9twK9-sJ"
      },
      "source": [
        "### Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8uONNrVbe08",
        "outputId": "39cd646d-ba57-4963-ae27-c6b3fa75cf8a"
      },
      "outputs": [],
      "source": [
        "!pip install -q redis langchain-community langchain-core langchain-google-genai tiktoken unstructured"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL1elOT86L2D"
      },
      "source": [
        "### Authenticate notebook to Google Cloud API\n",
        "\n",
        "You can get your Google Cloud API key at https://console.cloud.google.com/apis/credentials\n",
        "\n",
        "For security reason we recomment to restrict the key to allow only `Generative Language API`\n",
        "\n",
        "***Note - If you are participating in a workshop, your instructor should provide you with an API key.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dugRi01g7Gb",
        "outputId": "31855856-c376-4ce5-a023-7b2ced92d28a"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Provide your Google API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBlbUrB27QQs"
      },
      "source": [
        "### Install Redis Stack\n",
        "\n",
        "Redis will be used as the Vector Similarity Search engine for Langchain. This installs the Redis Stack database for local usage if you cannot or do not want to create a Redis Cloud database (https://redis.io/try-free/), and it installs the `redis-cli` that will be used for checking database connectivity, etc. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKMKXPY2j8Gt",
        "outputId": "02de0a0e-65bb-4fd3-ceb7-5863312042a1"
      },
      "outputs": [],
      "source": [
        "%%sh\n",
        "curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n",
        "echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n",
        "sudo apt-get update  > /dev/null 2>&1\n",
        "sudo apt-get install redis-stack-server  > /dev/null 2>&1\n",
        "redis-stack-server --daemonize yes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7UsU1Ts7TUL"
      },
      "source": [
        "### Setup Redis connection\n",
        "***Note - Replace Redis connection values if using a Redis Cloud instance!***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyPfCO3pkB7M",
        "outputId": "243feef6-a966-43fe-9881-600992fc2d33"
      },
      "outputs": [],
      "source": [
        "import redis\n",
        "import os\n",
        "\n",
        "REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\")\n",
        "REDIS_PORT = os.getenv(\"REDIS_PORT\", \"6379\")\n",
        "REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\", \"\")\n",
        "# Replace values above with your own if using Redis Cloud instance\n",
        "# REDIS_HOST=\"redis-18374.c253.us-central1-1.gce.cloud.redislabs.com\"\n",
        "# REDIS_PORT=18374\n",
        "# REDIS_PASSWORD=\"1TNxTEdYRDgIDKM2gDfasupCADXXXX\"\n",
        "\n",
        "# Shortcut for redis-cli $REDIS_CONN command\n",
        "# If SSL is enabled on the endpoint add --tls\n",
        "if REDIS_PASSWORD!=\"\":\n",
        "  os.environ[\"REDIS_CONN\"]=f\"-h {REDIS_HOST} -p {REDIS_PORT} -a {REDIS_PASSWORD} --no-auth-warning\"\n",
        "else:\n",
        "  os.environ[\"REDIS_CONN\"]=f\"-h {REDIS_HOST} -p {REDIS_PORT}\"\n",
        "\n",
        "# If SSL is enabled on the endpoint, use rediss:// as the URL prefix\n",
        "REDIS_URL = f\"redis://:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\"\n",
        "INDEX_NAME = f\"qna:idx\"\n",
        "\n",
        "# Test Redis connection\n",
        "!redis-cli $REDIS_CONN PING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h1e-L9yZfaY"
      },
      "outputs": [],
      "source": [
        "# Import Langchain dependencies\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.vectorstores.redis import Redis\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.chains import RetrievalQA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXDbABtw-vAQ"
      },
      "source": [
        "### Load text and split it into manageable chunks\n",
        "\n",
        "Without this step any large body of text would exceed the limit of tokens you can feed to the LLM. Vector search will return the most relevant chunks in our database to include in our prompt. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zfOlQeeZjsN",
        "outputId": "7b57b623-a98c-4cb6-9bb2-8fddd7aefcc9"
      },
      "outputs": [],
      "source": [
        "# Add your own URLs here\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/Redislabs-Solution-Architects/Redis-Workshops/main/resources/nvidia_q1fy25_earnings_call.txt\",\n",
        "    \"https://raw.githubusercontent.com/Redislabs-Solution-Architects/Redis-Workshops/main/resources/nvidia_q4fy24_earnings_call.txt\"\n",
        "]\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "documents = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index = True)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Optionally examine the result of text load+splitting\n",
        "# for text in texts:\n",
        "#  print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv1JVm31_OF9"
      },
      "source": [
        "### Initialize embeddings engine\n",
        "\n",
        "Here we are using Gemini embeddings engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaJrhuKa_Mwt"
      },
      "outputs": [],
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zgvqB6wCJWK"
      },
      "source": [
        "### Initialize LLM\n",
        "\n",
        "In this notebook we are using Gemini Pro LLM from Google Cloud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQRyyWU0CNbJ"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qrj-jeGmBRTL"
      },
      "source": [
        "### Create vector store from the documents using Redis as Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY69FViAjNv1"
      },
      "outputs": [],
      "source": [
        "def get_vectorstore() -> Redis:\n",
        "    \"\"\"Create the Redis vectorstore.\"\"\"\n",
        "\n",
        "    try:\n",
        "        vectorstore = Redis.from_existing_index(\n",
        "            embedding=embeddings,\n",
        "            index_name=INDEX_NAME,\n",
        "            redis_url=REDIS_URL\n",
        "        )\n",
        "        return vectorstore\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Load Redis with documents\n",
        "    vectorstore = Redis.from_documents(\n",
        "        documents=texts,\n",
        "        embedding=embeddings,\n",
        "        index_name=INDEX_NAME,\n",
        "        redis_url=REDIS_URL\n",
        "    )\n",
        "    return vectorstore\n",
        "\n",
        "redis = get_vectorstore()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdzQa112Bf2b"
      },
      "source": [
        "## Specify the prompt template\n",
        "\n",
        "PromptTemplate defines the exact text of the response that would be fed to the LLM. This step is optional, but the defaults usually work well for Gemini but might fall short for other models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKKn2KKp3TQ4"
      },
      "outputs": [],
      "source": [
        "def get_prompt():\n",
        "    \"\"\"Create the QA chain.\"\"\"\n",
        "    from langchain.prompts import PromptTemplate\n",
        "    from langchain.chains import RetrievalQA\n",
        "\n",
        "    # Define our prompt\n",
        "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, say that you don't know, don't try to make up an answer.\n",
        "\n",
        "    This should be in the following format:\n",
        "\n",
        "    Question: [question here]\n",
        "    Answer: [answer here]\n",
        "\n",
        "    Begin!\n",
        "\n",
        "    Context:\n",
        "    ---------\n",
        "    {context}\n",
        "    ---------\n",
        "    Question: {question}\n",
        "    Answer:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=prompt_template,\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgEXBujxG1dO"
      },
      "source": [
        "### Putting it all together\n",
        "\n",
        "This is where the Langchain brings all the components together in a form of a simple QnA chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKNSP0zqZq98"
      },
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=redis.as_retriever(search_type=\"similarity_distance_threshold\",search_kwargs={\"distance_threshold\":0.5}),\n",
        "    # return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": get_prompt()},\n",
        "    # verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ox7LffJ8VNe"
      },
      "source": [
        "### Debugging Redis\n",
        "\n",
        "The code block below is example of how you can interact with the Redis Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGH8mdG2na0w"
      },
      "outputs": [],
      "source": [
        "# !redis-cli $REDIS_CONN keys \"*\"\n",
        "# !redis-cli $REDIS_CONN HGETALL \"doc:qna:idx:2005b093c6af4f6c899779802910ed69\"\n",
        "\n",
        "###-- FLUSHDB will wipe out the entire database!!! Use with caution --###\n",
        "# !redis-cli $REDIS_CONN flushdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SURTtVbYBFGc"
      },
      "source": [
        "## Finally - let's ask questions!\n",
        "\n",
        "Examples:\n",
        "- Who is Jensen Huang?\n",
        "- How is the company diversifying? \n",
        "- How is Nvidia involved in the data center business? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0JkswfOHZu9h",
        "outputId": "8e345e54-8f46-4b5f-b31c-49044f2c517e"
      },
      "outputs": [],
      "source": [
        "query = \"Was Generative AI discussed, and if so, what was said?\"\n",
        "res=qa.invoke(query)\n",
        "res['result']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cleanup\n",
        "(Optional) Delete all keys and indexes from the database. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###-- FLUSHDB will wipe out the entire database!!! Use with caution --###\n",
        "# !redis-cli $REDIS_CONN flushdb"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
