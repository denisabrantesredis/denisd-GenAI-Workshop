{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N09X5wKjBIsT"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denisabrantesredis/denisd-GenAI-Workshop/blob/main/Labs/01-RAG_VectorDB_Cache/01_Redis_Langchain.ipynb\" target=\"_newt\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tk5ymdLBTeq"
      },
      "source": [
        "<div style=\"display:flex;width=100%;\">\n",
        "<img src=\"https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120\" alt=\"Redis\" width=\"90\"/>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<img src=\"https://www.gstatic.com/devrel-devsite/prod/v0e0f589edd85502a40d78d7d0825db8ea5ef3b99ab4070381ee86977c9168730/cloud/images/cloud-logo.svg\" alt=\"Google Cloud\" width=\"140\"/>\n",
        "</div>\n",
        "\n",
        "# Vector Similarity Search with Redis & Google Cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWGXHwYEOZoT"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/redis_gcp.png?raw=true\" alt=\"Redis and Google Cloud\" align=\"center\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8w5OAHNOlFf"
      },
      "source": [
        "[Try a similar app with an always-on demo](https://antonum-redis-vss-streamlit-streamlit-app-p4z5th.streamlit.app/)\n",
        "\n",
        "In this notebook, we will build a RAG use case using data from a web page. Redis will be used as the Vector Database and Cache for our use case, while Google Gemini is the LLM that will help generate the answers to the user's questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENspmGoTR6tq"
      },
      "source": [
        "## Installing the Pre-Reqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFmBBQ0bBFym"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers==3.0.1 >> /.tmp\n",
        "!pip install -q unstructured==0.15.10 >> /.tmp\n",
        "!pip install -q unstructured[pdf] >> /.tmp\n",
        "!pip install -q redis==5.0.8 >> /.tmp\n",
        "!pip install -q redisvl==0.3.5 >> /.tmp\n",
        "!pip install -q langchain==0.2.16 >> /.tmp\n",
        "!pip install -q langchain-core==0.3.6 >> /.tmp\n",
        "!pip install -q langchain-huggingface==0.0.3 >> /.tmp\n",
        "!pip install -q langchain-redis==0.0.4 >> /.tmp\n",
        "!pip install -q langchain-google-genai==2.0.0 >> /.tmp\n",
        "!pip install -q nltk==3.9.1 >> /.tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# patch an issue with RedisVL\n",
        "!wget https://github.com/denisabrantesredis/denisd-GenAI-Workshop/raw/refs/heads/main/_assets/files/semantic.py\n",
        "!rm /usr/local/lib/python3.10/dist-packages/redisvl/extensions/llmcache/semantic.py\n",
        "!cp semantic.py /usr/local/lib/python3.10/dist-packages/redisvl/extensions/llmcache/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7_A4sOeDfqt"
      },
      "source": [
        "## Part 1 - Declare a Document class to handle web site data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ6RwklAct5K"
      },
      "source": [
        "In this lab, we will use the [Unstructured](https://docs.unstructured.io/open-source/core-functionality/partitioning#partition-html) API to load data from a web page, parse it and break into chunks.\n",
        "\n",
        "A web page can have multiple different types of content; this class will help us identify the type of content being collected from the page, so we can make sure we're only getting the text from the page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ko4lH56BFs3"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional\n",
        "from enum import Enum\n",
        "from torch import Tensor\n",
        "from uuid import uuid4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD3zIcq6BFpx"
      },
      "outputs": [],
      "source": [
        "class DataType(str, Enum):\n",
        "    TITLE = \"Title\"\n",
        "    TEXT = \"Text\"\n",
        "    UNCATEGORIZED_TEXT = \"UncategorizedText\"\n",
        "    NARRATIVE_TEXT = \"NarrativeText\"\n",
        "    BULLETED_TEXT = \"BulletedText\"\n",
        "    PARAGRAPH = \"Paragraph\"\n",
        "    ABSTRACT = \"Abstract\"\n",
        "    THREADING = \"Threading\"\n",
        "    FORM = \"Form\"\n",
        "    FIELD_NAME = \"Field-Name\"\n",
        "    VALUE = \"Value\"\n",
        "    LINK = \"Link\"\n",
        "    COMPOSITE_ELEMENT = \"CompositeElement\"\n",
        "    IMAGE = \"Image\"\n",
        "    PICTURE = \"Picture\"\n",
        "    FIGURE_CAPTION = \"FigureCaption\"\n",
        "    FIGURE = \"Figure\"\n",
        "    CAPTION = \"Caption\"\n",
        "    LIST = \"List\"\n",
        "    LIST_ITEM = \"ListItem\"\n",
        "    LIST_ITEM_OTHER = \"List-item\"\n",
        "    CHECKED = \"Checked\"\n",
        "    UNCHECKED = \"Unchecked\"\n",
        "    CHECK_BOX_CHECKED = \"CheckBoxChecked\"\n",
        "    CHECK_BOX_UNCHECKED = \"CheckBoxUnchecked\"\n",
        "    RADIO_BUTTON_CHECKED = \"RadioButtonChecked\"\n",
        "    RADIO_BUTTON_UNCHECKED = \"RadioButtonUnchecked\"\n",
        "    ADDRESS = \"Address\"\n",
        "    EMAIL_ADDRESS = \"EmailAddress\"\n",
        "    PAGE_BREAK = \"PageBreak\"\n",
        "    FORMULA = \"Formula\"\n",
        "    TABLE = \"Table\"\n",
        "    HEADER = \"Header\"\n",
        "    HEADLINE = \"Headline\"\n",
        "    SUB_HEADLINE = \"Subheadline\"\n",
        "    PAGE_HEADER = \"Page-header\"  # Title?\n",
        "    SECTION_HEADER = \"Section-header\"\n",
        "    FOOTER = \"Footer\"\n",
        "    FOOTNOTE = \"Footnote\"\n",
        "    PAGE_FOOTER = \"Page-footer\"\n",
        "    PAGE_NUMBER = \"PageNumber\"\n",
        "    CODE_SNIPPET = \"CodeSnippet\"\n",
        "\n",
        "\n",
        "class Metadata(dict):\n",
        "    \"\"\"Metadata fields that pertain to the data source.\"\"\"\n",
        "    source: str\n",
        "    url: Optional[str] = None\n",
        "    text_as_html: Optional[str] = None\n",
        "\n",
        "\n",
        "class DataElement(dict):\n",
        "    \"\"\"A data element is a piece of text, image, link, or table.\"\"\"\n",
        "    \"\"\" The content field can contain text or Base64 encoded image data.\"\"\"\n",
        "    id: uuid4\n",
        "    data_type: DataType\n",
        "    content: str | bytes\n",
        "    metadata: Metadata\n",
        "    embeddings: Optional[Tensor] = None\n",
        "\n",
        "\n",
        "class Document(List[DataElement]):\n",
        "    \"\"\"A document is a list of data elements.\"\"\"\n",
        "    def from_dict(self, data: dict):\n",
        "        for element in data:\n",
        "            self.append(DataElement(\n",
        "                id=element[\"element_id\"],\n",
        "                data_type=DataType(element[\"type\"]),\n",
        "                content=element[\"text\"],\n",
        "                metadata=Metadata(\n",
        "                    source=element[\"metadata\"][\"source\"],\n",
        "                    url=element[\"metadata\"][\"url\"],\n",
        "                    text_as_html=element[\"metadata\"][\"text_as_html\"]\n",
        "                )\n",
        "            ))\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8Cq6TN1DrIk"
      },
      "source": [
        "## Part 2 - Extract text from the Web Site"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7QXZVQSDxlm"
      },
      "source": [
        "### Step 1: Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33RhGtaaduow"
      },
      "source": [
        "The process of 'parsing' or 'partitioning' will extract the text from the source (in this case, a web page), and group it into Elements.\n",
        "\n",
        "Since we're not interested in all the possible content in a web page, we will filter these elements so that we only capture `NarrativeText`, `List` and `ListItem` elements, meaning we will only get paragraphs and bullet-point lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHd7FtUPBFlE"
      },
      "outputs": [],
      "source": [
        "from unstructured.partition.html import partition_html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvoJfpupBFf2"
      },
      "outputs": [],
      "source": [
        "def parse(url):\n",
        "    print(f\"--> Starting parse: {url}\")\n",
        "    acceptable_types = [\"NarrativeText\", \"List\", \"ListItem\"]\n",
        "    elements = partition_html(url=url)\n",
        "    output_list = Document()\n",
        "    for element in elements:\n",
        "        el = element.to_dict()\n",
        "        el_type = el[\"type\"]\n",
        "        if el_type in acceptable_types:\n",
        "            if len(el[\"text\"]) >= 20:\n",
        "                output_list.append(element.to_dict())\n",
        "    print(f\"--> Total Elements: {len(output_list)}\")\n",
        "    return output_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZkC5BMHEBit"
      },
      "source": [
        "#### Define the web page you want to capture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp6tj60UeOcc"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_search.png?raw=true\" alt=\"Callout - Value can be changed\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-oiUVRLD3pm"
      },
      "outputs": [],
      "source": [
        "blog_page = parse(\"https://redis.io/blog/redis-insight-makes-rdi-even-simpler/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJdvZR-FEIcE"
      },
      "outputs": [],
      "source": [
        "blog_page[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r2BAmmYFRFW"
      },
      "source": [
        "### Step 2: Chunking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78HbXDWyguH1"
      },
      "source": [
        "Chunking is the process of grouping elements together into more meaningful text blocks for vector generation.\n",
        "\n",
        "Due to time and scope constraints, this lab will use a very simple chunking function. However, it's important to keep in mind that this is one of the most important steps in a RAG implementation; a well-designed (and tested) chunking strategy is vital for the success of your RAG project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7JU1jbuFS7C"
      },
      "outputs": [],
      "source": [
        "from unstructured.chunking.title import chunk_by_title\n",
        "from unstructured.staging.base import convert_to_dict, dict_to_elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UXXqvcsFWmd"
      },
      "outputs": [],
      "source": [
        "def chunk_docs_unstruct(elements):\n",
        "    chunking_settings = {\n",
        "        \"combine_text_under_n_chars\": 50,\n",
        "        \"max_characters\": 750,\n",
        "        \"new_after_n_chars\": 500\n",
        "    }\n",
        "    chunked_raw = chunk_by_title(elements=elements, **chunking_settings)\n",
        "    results = convert_to_dict(chunked_raw)\n",
        "    return results\n",
        "\n",
        "\n",
        "def chunk(input_data):\n",
        "    print(f\"--> Generating Chunks\")\n",
        "    elements_raw = dict_to_elements(input_data)\n",
        "    elements = chunk_docs_unstruct(elements_raw)\n",
        "    print(f\"--> Generated {len(elements)} chunks\")\n",
        "    return elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pmy8z6nWFbR7"
      },
      "outputs": [],
      "source": [
        "chunked_page = chunk(blog_page)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYwG0oSjFeuP"
      },
      "outputs": [],
      "source": [
        "chunked_page[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j-_YodJFyZH"
      },
      "source": [
        "## Part 3: Generating the Vectors & Saving to Redis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGd2WQ9LjK_M"
      },
      "source": [
        "In this lab, we will leverage the Langchain package for Redis, which automates most of the functions required to setup and use Redis as Vector Database and Cache service.\n",
        "\n",
        "To learn more about the Langchain package for Redis, visit the official documentation: [https://python.langchain.com/docs/integrations/vectorstores/redis/](https://python.langchain.com/docs/integrations/vectorstores/redis/)\n",
        "\n",
        "To generate the embeddings, we will use the [Huggingface embedding model](https://python.langchain.com/docs/integrations/text_embedding/huggingfacehub/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdCicFUVkb-p"
      },
      "source": [
        "### Importing Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7tb6ImZF247"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import redis\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"PYDANTIC_SKIP_VALIDATING_CORE_SCHEMAS\"] = \"True\"\n",
        "\n",
        "from langchain_redis import RedisConfig, RedisVectorStore\n",
        "from langchain_huggingface import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQkY7YrLkUZD"
      },
      "source": [
        "### Step 1: Setting Up Connection String"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfApethtkegP"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_secrets.png?raw=true\" alt=\"Callout - Use Google Colab secrets instead\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTHiBGT_GYac"
      },
      "outputs": [],
      "source": [
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    if userdata.get('GOOGLE_API_KEY'):\n",
        "      os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "    else:\n",
        "      os.environ[\"GOOGLE_API_KEY\"] = \"<insert API key here>\"\n",
        "\n",
        "if userdata.get('REDIS_HOST'):\n",
        "  REDIS_HOST = userdata.get('REDIS_HOST')\n",
        "else:\n",
        "  REDIS_HOST=\"127.0.0.1\"\n",
        "\n",
        "if userdata.get('REDIS_PORT'):\n",
        "  REDIS_PORT = userdata.get('REDIS_PORT')\n",
        "else:\n",
        "  REDIS_PORT=12000\n",
        "\n",
        "if userdata.get('REDIS_PASSWORD'):\n",
        "  REDIS_PASSWORD = userdata.get('REDIS_PASSWORD')\n",
        "else:\n",
        "  REDIS_PASSWORD=\"password\"\n",
        "\n",
        "REDIS_URL = f\"redis://default:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKgHfuHnlcXh"
      },
      "source": [
        "#### Testing the Connection to Redis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmj6JoY0letv"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_connection.png?raw=true\" alt=\"Callout - Make sure connection works\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMAp-8M-j8yo"
      },
      "outputs": [],
      "source": [
        "r = redis.from_url(REDIS_URL)\n",
        "\n",
        "if r.ping():\n",
        "    print(\"Connection successful!\")\n",
        "else:\n",
        "    print(\"Connection issue!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvIQKclfl-3S"
      },
      "source": [
        "The Redis configuration includes the name of the index that will be used for Vector search. This index is created automatically by the Langchain package, while allowing developers to control the additional metadata that will be stored with the vectors (for hybrid searches)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCjcJIrsGlqI"
      },
      "outputs": [],
      "source": [
        "embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "config = RedisConfig(\n",
        "    index_name=\"idx:web\",\n",
        "    redis_url=REDIS_URL,\n",
        "    metadata_schema=[\n",
        "        {\"name\": \"id\", \"type\": \"text\"},\n",
        "        {\"name\": \"url\", \"type\": \"text\"},\n",
        "        {\"name\": \"filetype\", \"type\": \"text\"},\n",
        "        {\"name\": \"languages\", \"type\": \"tag\"}\n",
        "    ]\n",
        ")\n",
        "vector_store = RedisVectorStore(embeddings, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq-_RvOKmVr7"
      },
      "source": [
        "### Step 2 - Add the chunks to a JSON list and store in Redis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYRtnOltmyC4"
      },
      "source": [
        "In this step, we prepare a list of JSON objects contanining the data from our chunks. Here is where we can map the metadata fields we want to store in Redis to be used in hybrid searches. Notice how we are not generating the vectors manually as part of the step; this is fully automated by the Langchain package, based on the embedding model we've selected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0H9huxKHI4q"
      },
      "outputs": [],
      "source": [
        "counter = 0\n",
        "texts = []\n",
        "metadata = []\n",
        "\n",
        "for document in chunked_page:\n",
        "    counter = counter + 1\n",
        "    texts.append(document['text'])\n",
        "    metadata_obj = {\n",
        "                       \"id\": f\"webdoc:{counter:05}\",\n",
        "                       \"url\": document[\"metadata\"][\"url\"],\n",
        "                       \"filetype\": document[\"metadata\"][\"filetype\"],\n",
        "                       \"languages\": document[\"metadata\"][\"languages\"],\n",
        "                    }\n",
        "    metadata.append(metadata_obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Y3Fy_JnSm5"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_save.png?raw=true\" alt=\"Callout - Saving to Redis\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLUdjv5KHOpJ"
      },
      "outputs": [],
      "source": [
        "timer_start = time.perf_counter()\n",
        "ids = vector_store.add_texts(texts, metadata)\n",
        "timer_end = time.perf_counter()\n",
        "total_time = round(timer_end - timer_start, 4)\n",
        "print(f\"Total Time: {total_time}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_j8oVZIzgvv"
      },
      "source": [
        "List the IDs of all documents saved to Redis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g-MMhPNze4Z"
      },
      "outputs": [],
      "source": [
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jHy0dVgsAAg"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDX0QOboqped"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_insight.png?raw=true\" alt=\"Callout - Check Redis Insight\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxBu0RKZqu4t"
      },
      "source": [
        "Open Redis Insight and confirm that all documents were generated. Notice how each document contains the vector that was automatically generated by the Langchain package. You may also notice that the vectors are not presented as a list; this is due to the fact that they are stored as binary strings, which is more efficient for retrieval and storage.\n",
        "\n",
        "You can also go to the **Workbench** and get a list of indexes using the command:\n",
        "\n",
        "```\n",
        "FT._list\n",
        "```\n",
        "\n",
        "Finally, you can get more details about the index that was automatically generated by Langchain with this command:\n",
        "```\n",
        "FT.info \"idx:web\"\n",
        "```\n",
        "&nbsp;\n",
        "\n",
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__zfLJoaHa13"
      },
      "source": [
        "## Part 4: Running a Vector Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjpqcmh8vGFP"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_question.png?raw=true\" alt=\"Callout - Change Question\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "249O38igHeb1"
      },
      "outputs": [],
      "source": [
        "query = \"How does Redis Insight make RDI simpler?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS3kewVQvR8y"
      },
      "source": [
        "### Running a Semantic Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4WSqBSVvZCV"
      },
      "source": [
        "The Langchain integration greatly simplifies the process of running a semantic search. A single function call is enough. Notice how we do not need to generate a vector for our question manually; this is handled automatically by the function, based on the embedding model we've selected before.\n",
        "\n",
        "For more details on the different ways to run vector searches, check the [Langchain documentation page](https://python.langchain.com/docs/integrations/vectorstores/redis/#query-vector-store).\n",
        "\n",
        "&nbsp;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-n9ER0lHi12"
      },
      "outputs": [],
      "source": [
        "timer_start = time.perf_counter()\n",
        "results = vector_store.similarity_search_with_score(query)\n",
        "timer_end = time.perf_counter()\n",
        "total_time = round(timer_end - timer_start, 4)\n",
        "print(f\"Total Time: {total_time}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FKaU45iwULg"
      },
      "source": [
        "### Visualizing the search results with the score for each result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wepEg9G0umFH"
      },
      "outputs": [],
      "source": [
        "print(f\"Search results for '{query}':\")\n",
        "for doc in results:\n",
        "    print(\"----\")\n",
        "    print(f\"Score: {doc[1]} - {doc[0].page_content} (Source: {doc[0].metadata['url']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZUaPXegyFyx"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UALXxSxHHugw"
      },
      "source": [
        "## Part 5: Using a LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC3h0h55yKKJ"
      },
      "source": [
        "In this lab, we will use the Gemini Pro 1.5 model from Google to generate a response to the user, based on the documents retrieved from Redis. The GCP API Key that we set before is required to allow access to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3UgzWI31G6P"
      },
      "source": [
        "### Step 1: Load the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAMSsoWgHw2u"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDNxn4XlIc6T"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    top_k=64,\n",
        "    max_output_tokens=8192\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dazMdtlQIi6Z"
      },
      "source": [
        "### Step 2: Prepare a list with the text from the documents retrieved by the vector search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODF9JrNwz7Pu"
      },
      "source": [
        "We will ask the model to respond to the user's questions. To help with the answer, we want to provide the text from the documents that were retrieved by the semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vptTC1f8Imo7"
      },
      "outputs": [],
      "source": [
        "text_list = []\n",
        "distance_list = []\n",
        "\n",
        "for node in results:\n",
        "    text_list.append(node[0].page_content)\n",
        "    distance = node[1]\n",
        "    distance_list.append(distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnQaXkij0GsC"
      },
      "source": [
        "Print the list that will be sent to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ0cxZ3AIpax"
      },
      "outputs": [],
      "source": [
        "text_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wyvpWAdIscB"
      },
      "source": [
        "### Step 3: Prepare the Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBeR4rv80Qhr"
      },
      "source": [
        "Since this is just a lab, we will keep the prompt very simple, with just basic instructions for the model to answer based on the documents from the semantic search, and to stick to the documents for the response. Production prompts will benefit from more sophisticated prompts, as well as other controls like guardrails, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HJjxxOcIuGy"
      },
      "outputs": [],
      "source": [
        "def get_system_template(text_list, query):\n",
        "  system_template = \"\"\"\n",
        "  Your task is to answer questions by using a given context.\n",
        "\n",
        "  Don't invent anything that is outside of the context.\n",
        "\n",
        "  %CONTEXT%\n",
        "  {context}\n",
        "\n",
        "  \"\"\"\n",
        "  messages = [\n",
        "      SystemMessage(content=system_template.format(context=text_list)),\n",
        "      HumanMessage(content=query)\n",
        "  ]\n",
        "\n",
        "  return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaezsB6k0zxv"
      },
      "outputs": [],
      "source": [
        "messages = get_system_template(text_list, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FtObK5_Iwmq"
      },
      "source": [
        "### Step 4: Invoke the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQjHLVPk5tL7"
      },
      "source": [
        "Since we are not using Redis as cache, the model will be called every time, even if the same question (or a similar) is asked multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QpDbPJCIzGE"
      },
      "outputs": [],
      "source": [
        "timer_start = time.perf_counter()\n",
        "llm_response = llm.invoke(messages)\n",
        "timer_end = time.perf_counter()\n",
        "total_time = round(timer_end - timer_start, 4)\n",
        "print(f\"Total Time: {total_time}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZjMFpOF1AOY"
      },
      "source": [
        "Visualizing the model response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeCGJit20POh"
      },
      "outputs": [],
      "source": [
        "llm_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX9qkKbB526_"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh-IMpo8JdB7"
      },
      "source": [
        "## Part 6: Leveraging Redis for Basic Cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFYrtKhb55KA"
      },
      "source": [
        "Redis can be used not only as the Vector Database, but also as a cache to store responses from the Large Language Model, which can significantly improve user experience, by retrieving responses in milliseconds instead of seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4kBM-GDJg4h"
      },
      "outputs": [],
      "source": [
        "from langchain_redis import RedisCache\n",
        "from langchain.globals import set_llm_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apl2cosa6oLR"
      },
      "source": [
        "To use Redis as a cache, we only need 2 lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtWde9w_J0sQ"
      },
      "outputs": [],
      "source": [
        "redis_cache = RedisCache(redis_url=REDIS_URL)\n",
        "set_llm_cache(redis_cache)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eEryVNs6qay"
      },
      "source": [
        "We will repeat the same question from before. Since we have just enabled the cache, it will be empty, which means that this next question will require a vector search and will need to go through the Large Language Model again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OGmcuaaKF75"
      },
      "outputs": [],
      "source": [
        "query = \"How does Redis Insight make RDI simpler?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt_wqC2yKM3Y"
      },
      "outputs": [],
      "source": [
        "timer_start = time.perf_counter()\n",
        "result_nodes = vector_store.similarity_search_with_score(query)\n",
        "timer_end = time.perf_counter()\n",
        "total_time = round(timer_end - timer_start, 4)\n",
        "print(f\"Total Time: {total_time}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfgHUnJx7EqP"
      },
      "source": [
        "Prepare the list of texts to send to the LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChKJ5TivKWeA"
      },
      "outputs": [],
      "source": [
        "text_list = []\n",
        "distance_list = []\n",
        "\n",
        "for node in result_nodes:\n",
        "    text_list.append(node[0].page_content)\n",
        "    distance = node[1]\n",
        "    distance_list.append(distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs9QcQN97I_g"
      },
      "source": [
        "Display the search results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBHp5BiHKjIp"
      },
      "outputs": [],
      "source": [
        "print(f\"--> Total Documents Found: {len(result_nodes)}\")\n",
        "for node in result_nodes:\n",
        "  print(f\"--> {node[1]} | {node[0].page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBH1nheo7L7v"
      },
      "source": [
        "Prepare the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JBZHBpKMIV_"
      },
      "outputs": [],
      "source": [
        "messages = get_system_template(text_list, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkKG8oR17OKU"
      },
      "source": [
        "Call the model (the `invoke` function will check and populate the cache automatically):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7at9YHOWKG6"
      },
      "outputs": [],
      "source": [
        "timer_start = time.perf_counter()\n",
        "llm_response = llm.invoke(messages)\n",
        "timer_end = time.perf_counter()\n",
        "total_time = round(timer_end - timer_start, 4)\n",
        "print(f\"Total Time: {total_time}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URIIB4Hy7WrH"
      },
      "source": [
        "Print the LLM response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipERjc1aRcCC"
      },
      "outputs": [],
      "source": [
        "llm_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6oBBqwS7haw"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJakv8xO7i8Q"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_insight.png?raw=true\" alt=\"Callout - Check Redis Insight\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp36z0d-7hR7"
      },
      "source": [
        "A new document should appear on Redis, of type JSON. This is the cached response from the LLM.\n",
        "Notice that the key is made from a long this; this is a hash of the question.\n",
        "\n",
        "Because this is a basic cache, questions from the user will be hashed and compared against the key, which means that for this basic cache, questions must match exactly in order to be used.\n",
        "\n",
        "&nbsp;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gemlQkpZNLx-"
      },
      "source": [
        "#### Repeating the question to fetch results from the cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvHXlFXL8y5Z"
      },
      "source": [
        "When we ask exactly the same question as before, it should trigger a cache hit, meaning we will receive the answer from the Redis cache, much faster than calling the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qV4r9h-9NOcP"
      },
      "outputs": [],
      "source": [
        "timer_start = time.perf_counter()\n",
        "llm_response = llm.invoke(messages)\n",
        "timer_end = time.perf_counter()\n",
        "total_time = round(timer_end - timer_start, 4)\n",
        "print(f\"Total Time: {total_time}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btlVe7qM9G7T"
      },
      "source": [
        "Print the cached response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDGRpcHORY93"
      },
      "outputs": [],
      "source": [
        "llm_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4sszbPX9KXm"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4R86mhmNhuT"
      },
      "source": [
        "#### Asking the same question (worded differently) will cause a cache miss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZmHsS8t9MkC"
      },
      "source": [
        "If the question is not an exact match, it will cause a cache miss. This might be an issue with most of the RAG use cases, which is why we will be exploring Semantic Cache next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhRX_GoHNmuW"
      },
      "outputs": [],
      "source": [
        "# original query = \"How does Redis Insight make RDI simpler?\"\n",
        "query = \"What does Redis Insight do to make RDI simpler?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHQq4Shr_ZKf"
      },
      "source": [
        "Prepare the prompt with the new query (PS: we're skipping the vector search on purpose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCFhpjBgNwVa"
      },
      "outputs": [],
      "source": [
        "messages = get_system_template(text_list, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_JTMxC8_i9E"
      },
      "source": [
        "Call the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hogIqAOnNwQh"
      },
      "outputs": [],
      "source": [
        "timer_start = time.perf_counter()\n",
        "llm_response = llm.invoke(messages)\n",
        "timer_end = time.perf_counter()\n",
        "total_time = round(timer_end - timer_start, 4)\n",
        "print(f\"Total Time: {total_time}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9W1P_M6_mDp"
      },
      "source": [
        "Print the response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzO7OAVoNwJt"
      },
      "outputs": [],
      "source": [
        "llm_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPVB6DDX_1LG"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbNoHMtFN45X"
      },
      "source": [
        "## Part 7 - Leveraging Redis for Semantic Cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwR5FEo2AJjK"
      },
      "source": [
        "The Semantic Cache will generate vectors for each prompt, and store the response from the LLM. That way, new prompts are converted into vectors automatically and a semantic search is executed on Redis, looking for similar questions.\n",
        "\n",
        "It is possible to set the threshold for for semantic search; for this lab, we are using 20%. In your project, you can run multiple tests with different thresholds, to determined what works best for your use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFuDm3vwOCJg"
      },
      "outputs": [],
      "source": [
        "from langchain_redis import RedisSemanticCache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv75yqFrAqry"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_threshold.png?raw=true\" alt=\"Callout - Semantic Threshold\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA-yaqYtN-bR"
      },
      "outputs": [],
      "source": [
        "redis_cache = RedisSemanticCache(redis_url=REDIS_URL, embeddings=embeddings, distance_threshold=0.2)\n",
        "set_llm_cache(redis_cache)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv6nR-AFBEVZ"
      },
      "source": [
        "Since the Semantic Cache is new, it will be empty. We will ask the original question first, to generate the cache entry:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm3SLk2u-SWn"
      },
      "outputs": [],
      "source": [
        "query = \"How does Redis Insight make RDI simpler?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjeCV10VBMD8"
      },
      "source": [
        "Prepare the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3Pvy352-Rsb"
      },
      "outputs": [],
      "source": [
        "messages = get_system_template(text_list, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWKkgnnoBO-d"
      },
      "source": [
        "Invoke the model (it will cause a cache miss):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJGYVojmWUdB"
      },
      "outputs": [],
      "source": [
        "timer_start = time.perf_counter()\n",
        "llm_response = llm.invoke(messages)\n",
        "timer_end = time.perf_counter()\n",
        "total_time = round(timer_end - timer_start, 4)\n",
        "print(f\"Total Time: {total_time}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--0PfUm1BUa8"
      },
      "source": [
        "Print the response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heR894L4-lHd"
      },
      "outputs": [],
      "source": [
        "llm_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNVN4Q2MBYOQ"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoqlHw8DO0zL"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_insight.png?raw=true\" alt=\"Callout - Check Redis Insight\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoO1iZmVO1zW"
      },
      "source": [
        "A new Hash document will appear in Redis, with a key prefix of `llmcache`. This is the cached prompt, which includes the question and the answer. The `invoke` function will run a semantic search for these documents, to look for similar questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDfgdGQnO_ci"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3iZspjDQc_b"
      },
      "source": [
        "#### Ask a similar question to trigger a cache hit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1dac3x9QV-h"
      },
      "outputs": [],
      "source": [
        "query = \"What does Redis Insight do to make RDI simpler?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5WV9CfJBedI"
      },
      "source": [
        "Prepare the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6ekUthmQXVh"
      },
      "outputs": [],
      "source": [
        "messages = get_system_template(text_list, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1Syj2_WBgTC"
      },
      "source": [
        "Invoke the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNeUsew8QXSu"
      },
      "outputs": [],
      "source": [
        "timer_start = time.perf_counter()\n",
        "llm_response = llm.invoke(messages)\n",
        "timer_end = time.perf_counter()\n",
        "total_time = round(timer_end - timer_start, 4)\n",
        "print(f\"Total Time: {total_time}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e78ba7Y5Bi66"
      },
      "source": [
        "Print the response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt_hlPsRQXQE"
      },
      "outputs": [],
      "source": [
        "llm_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAVzG-35N0JI"
      },
      "source": [
        "&nbsp;\n",
        "\n",
        "\n",
        "&nbsp;\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G4eIa8hN2T9"
      },
      "source": [
        "# Congrats, this is the end of the lab!!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
