{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N09X5wKjBIsT"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denisabrantesredis/denisd-GenAI-Workshop/blob/main/Labs/03-RAG_Images/03_Redis_RAG_Images.ipynb\" target=\"_newt\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tk5ymdLBTeq"
      },
      "source": [
        "<div style=\"display:flex;width=100%;\">\n",
        "<img src=\"https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120\" alt=\"Redis\" width=\"90\"/>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<img src=\"https://www.gstatic.com/devrel-devsite/prod/v0e0f589edd85502a40d78d7d0825db8ea5ef3b99ab4070381ee86977c9168730/cloud/images/cloud-logo.svg\" alt=\"Google Cloud\" width=\"140\"/>\n",
        "</div>\n",
        "\n",
        "# Vector Similarity Search with Redis & Google Cloud - RAG for Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWGXHwYEOZoT"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/redis_gcp.png?raw=true\" alt=\"Redis and Google Cloud\" align=\"center\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8w5OAHNOlFf"
      },
      "source": [
        "[Try a similar app with an always-on demo](https://ecommerce.redisventures.com/)\n",
        "\n",
        "In this notebook, we will build a RAG use case using data from a web page. Redis will be used as the Vector Database and Cache for our use case, while Google Gemini is the LLM that will help generate the answers to the user's questions.\n",
        "\n",
        "The dataset for this lab contains images of products like shoes, watches, clothes, etc. We will use Google Gemini to provide a description of each product that we can use as metadata."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENspmGoTR6tq"
      },
      "source": [
        "## Installing the Pre-Reqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFmBBQ0bBFym"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers==3.0.1 >> /.tmp\n",
        "!pip install -q redis==5.0.8 >> /.tmp\n",
        "!pip install -q redisvl==0.3.5 >> /.tmp\n",
        "!pip install -q langchain==0.2.16 >> /.tmp\n",
        "!pip install -q langchain-core==0.3.6 >> /.tmp\n",
        "!pip install -q langchain-huggingface==0.0.3 >> /.tmp\n",
        "!pip install -q langchain-redis==0.0.4 >> /.tmp\n",
        "!pip install -q langchain-google-genai==2.0.0 >> /.tmp\n",
        "!pip install -q langchain_experimental==0.3.2 >> /.tmp\n",
        "!pip install -q open-clip-torch==2.26.1 >> /.tmp\n",
        "!pip install -q git+https://github.com/openai/CLIP.git >> /.tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# patch an issue with RedisVL\n",
        "!wget https://github.com/denisabrantesredis/denisd-GenAI-Workshop/raw/refs/heads/main/_assets/files/semantic.py\n",
        "!rm /usr/local/lib/python3.10/dist-packages/redisvl/extensions/llmcache/semantic.py\n",
        "!cp semantic.py /usr/local/lib/python3.10/dist-packages/redisvl/extensions/llmcache/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installing Redis Stack Locally\n",
        "If you are not using Redis Cloud as a database, uncomment and run the code below to install Redis locally. Then set your connection to 127.0.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%sh\n",
        "# curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg \n",
        "# echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list \n",
        "# sudo apt-get update  > /dev/null 2>&1\n",
        "# sudo apt-get install redis-stack-server  > /dev/null 2>&1\n",
        "# redis-stack-server --daemonize yes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ6RwklAct5K"
      },
      "source": [
        "### Loading Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ko4lH56BFs3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import redis\n",
        "import torch\n",
        "import base64\n",
        "import random\n",
        "from typing import Any, Dict\n",
        "from IPython.display import Image\n",
        "from google.colab import userdata\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field, model_serializer\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8Cq6TN1DrIk"
      },
      "source": [
        "## Part 1 - Prepare the Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7QXZVQSDxlm"
      },
      "source": [
        "### Step 1: Download Dataset from Github"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33RhGtaaduow"
      },
      "source": [
        "For performance reasons, we will only be working with 20 images. In this github repo, you can find a zip file with 100 images that can be used instead, if you want to test this code in an environment with more resources, like Google Vertex AI.\n",
        "\n",
        "We will download the images that will be stored in Redis as vectors, as well as a smaller dataset of images that will be used for semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHd7FtUPBFlE"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"./img_20\"):\n",
        "  !wget https://github.com/denisabrantesredis/denisd-GenAI-Workshop/raw/refs/heads/main/_assets/files/img20.zip\n",
        "  !unzip img20.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvoJfpupBFf2"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"./img_search_20\"):\n",
        "  !wget https://github.com/denisabrantesredis/denisd-GenAI-Workshop/raw/refs/heads/main/_assets/files/img_search20.zip\n",
        "  !unzip img_search20.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQkY7YrLkUZD"
      },
      "source": [
        "### Step 2: Setting up the Redis connection and GCP API Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfApethtkegP"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_secrets.png?raw=true\" alt=\"Callout - Use Google Colab secrets instead\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTHiBGT_GYac"
      },
      "outputs": [],
      "source": [
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    if userdata.get('GOOGLE_API_KEY'):\n",
        "      os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "    else:\n",
        "      os.environ[\"GOOGLE_API_KEY\"] = \"<insert API key here>\"\n",
        "\n",
        "if userdata.get('REDIS_HOST'):\n",
        "  REDIS_HOST = userdata.get('REDIS_HOST')\n",
        "else:\n",
        "  REDIS_HOST=\"127.0.0.1\"\n",
        "\n",
        "if userdata.get('REDIS_PORT'):\n",
        "  REDIS_PORT = userdata.get('REDIS_PORT')\n",
        "else:\n",
        "  REDIS_PORT=12000\n",
        "\n",
        "if userdata.get('REDIS_PASSWORD'):\n",
        "  REDIS_PASSWORD = userdata.get('REDIS_PASSWORD')\n",
        "else:\n",
        "  REDIS_PASSWORD=\"password\"\n",
        "\n",
        "REDIS_URL = f\"redis://default:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKgHfuHnlcXh"
      },
      "source": [
        "#### Testing the Connection to Redis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmj6JoY0letv"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_connection.png?raw=true\" alt=\"Callout - Make sure connection works\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMAp-8M-j8yo"
      },
      "outputs": [],
      "source": [
        "r = redis.from_url(REDIS_URL)\n",
        "\n",
        "if r.ping():\n",
        "    print(\"Connection successful!\")\n",
        "else:\n",
        "    print(\"Connection issue!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvIQKclfl-3S"
      },
      "source": [
        "### Step 3: Load the list of images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This lab can greatly benefit from running on a T4 GPU. However, seeing as GPUs are not guaranteed in the free tier, the lab was designed to also run on CPUs, albeit slower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCjcJIrsGlqI"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0H9huxKHI4q"
      },
      "outputs": [],
      "source": [
        "filenames = glob.glob(\"./img_20/*\")\n",
        "len(filenames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 4: Load Gemini and get it to describe an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    top_k=64,\n",
        "    max_output_tokens=8192\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the image that will be sent to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(filenames[0], \"rb\") as image_file:\n",
        "    image_data = base64.b64encode(image_file.read()).decode()\n",
        "\n",
        "Image(filenames[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Y3Fy_JnSm5"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_geminiimage.png?raw=true\" alt=\"Callout - Upload Image to Gemini\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLUdjv5KHOpJ"
      },
      "outputs": [],
      "source": [
        "message = HumanMessage(\n",
        "    content=[\n",
        "        {\"type\": \"text\", \"text\": \"describe the object in this image\"},\n",
        "        {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n",
        "        },\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_j8oVZIzgvv"
      },
      "source": [
        "Call the model and print the response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g-MMhPNze4Z"
      },
      "outputs": [],
      "source": [
        "# Invoke the model with the message\n",
        "response = llm.invoke([message])\n",
        "\n",
        "# Print the model's response\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jHy0dVgsAAg"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__zfLJoaHa13"
      },
      "source": [
        "## Part 2: Categorizing the Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjpqcmh8vGFP"
      },
      "source": [
        "Usually, a dataset of images would have a curated set of metadata attributes, that would be used as metadata for hybrid searches. In this lab, however, we will generate metadata for each image using Google Gemini to provide a description of the image and its key characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS3kewVQvR8y"
      },
      "source": [
        "### Prepare a list of images on base64 format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-n9ER0lHi12"
      },
      "outputs": [],
      "source": [
        "image_list = []\n",
        "for i in range(len(filenames)):\n",
        "    this_image_path = filenames[i]\n",
        "    with open(this_image_path, \"rb\") as image_file:\n",
        "        image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "        image_list.append(image_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FKaU45iwULg"
      },
      "source": [
        "### Visualizing the search results with the score for each result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wepEg9G0umFH"
      },
      "outputs": [],
      "source": [
        "print(f\"Search results for '{query}':\")\n",
        "for doc in results:\n",
        "    print(\"----\")\n",
        "    print(f\"Score: {doc[1]} - {doc[0].page_content} (Source: {doc[0].metadata['url']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZUaPXegyFyx"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UALXxSxHHugw"
      },
      "source": [
        "## Part 5: Using a LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC3h0h55yKKJ"
      },
      "source": [
        "In this lab, we will use the Gemini Pro 1.5 model from Google to generate a response to the user, based on the documents retrieved from Redis. The GCP API Key that we set before is required to allow access to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3UgzWI31G6P"
      },
      "source": [
        "### Step 1: Load the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAMSsoWgHw2u"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDNxn4XlIc6T"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    top_k=64,\n",
        "    max_output_tokens=8192\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dazMdtlQIi6Z"
      },
      "source": [
        "### Step 2: Prepare a list with the text from the documents retrieved by the vector search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODF9JrNwz7Pu"
      },
      "source": [
        "We will ask the model to respond to the user's questions. To help with the answer, we want to provide the text from the documents that were retrieved by the semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vptTC1f8Imo7"
      },
      "outputs": [],
      "source": [
        "text_list = []\n",
        "distance_list = []\n",
        "\n",
        "for node in results:\n",
        "    text_list.append(node[0].page_content)\n",
        "    distance = node[1]\n",
        "    distance_list.append(distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnQaXkij0GsC"
      },
      "source": [
        "Print the list that will be sent to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ0cxZ3AIpax"
      },
      "outputs": [],
      "source": [
        "text_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wyvpWAdIscB"
      },
      "source": [
        "### Step 3: Prepare the Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBeR4rv80Qhr"
      },
      "source": [
        "Since this is just a lab, we will keep the prompt very simple, with just basic instructions for the model to answer based on the documents from the semantic search, and to stick to the documents for the response. Production prompts will benefit from more sophisticated prompts, as well as other controls like guardrails, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HJjxxOcIuGy"
      },
      "outputs": [],
      "source": [
        "def get_system_template(text_list, query):\n",
        "  system_template = \"\"\"\n",
        "  Your task is to answer questions by using a given context.\n",
        "\n",
        "  Don't invent anything that is outside of the context.\n",
        "\n",
        "  %CONTEXT%\n",
        "  {context}\n",
        "\n",
        "  \"\"\"\n",
        "  messages = [\n",
        "      SystemMessage(content=system_template.format(context=text_list)),\n",
        "      HumanMessage(content=query)\n",
        "  ]\n",
        "\n",
        "  return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaezsB6k0zxv"
      },
      "outputs": [],
      "source": [
        "messages = get_system_template(text_list, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FtObK5_Iwmq"
      },
      "source": [
        "### Step 4: Invoke the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQjHLVPk5tL7"
      },
      "source": [
        "Since we are not using Redis as cache, the model will be called every time, even if the same question (or a similar) is asked multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QpDbPJCIzGE"
      },
      "outputs": [],
      "source": [
        "timer_start = time.perf_counter()\n",
        "llm_response = llm.invoke(messages)\n",
        "timer_end = time.perf_counter()\n",
        "total_time = round(timer_end - timer_start, 4)\n",
        "print(f\"Total Time: {total_time}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZjMFpOF1AOY"
      },
      "source": [
        "Visualizing the model response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeCGJit20POh"
      },
      "outputs": [],
      "source": [
        "llm_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX9qkKbB526_"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh-IMpo8JdB7"
      },
      "source": [
        "## Part 6: Leveraging Redis for Basic Cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFYrtKhb55KA"
      },
      "source": [
        "Redis can be used not only as the Vector Database, but also as a cache to store responses from the Large Language Model, which can significantly improve user experience, by retrieving responses in milliseconds instead of seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4kBM-GDJg4h"
      },
      "outputs": [],
      "source": [
        "from langchain_redis import RedisCache\n",
        "from langchain.globals import set_llm_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apl2cosa6oLR"
      },
      "source": [
        "To use Redis as a cache, we only need 2 lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtWde9w_J0sQ"
      },
      "outputs": [],
      "source": [
        "redis_cache = RedisCache(redis_url=REDIS_URL)\n",
        "set_llm_cache(redis_cache)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eEryVNs6qay"
      },
      "source": [
        "We will repeat the same question from before. Since we have just enabled the cache, it will be empty, which means that this next question will require a vector search and will need to go through the Large Language Model again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OGmcuaaKF75"
      },
      "outputs": [],
      "source": [
        "query = \"How does Redis Insight make RDI simpler?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt_wqC2yKM3Y"
      },
      "outputs": [],
      "source": [
        "timer_start = time.perf_counter()\n",
        "result_nodes = vector_store.similarity_search_with_score(query)\n",
        "timer_end = time.perf_counter()\n",
        "total_time = round(timer_end - timer_start, 4)\n",
        "print(f\"Total Time: {total_time}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfgHUnJx7EqP"
      },
      "source": [
        "Prepare the list of texts to send to the LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChKJ5TivKWeA"
      },
      "outputs": [],
      "source": [
        "text_list = []\n",
        "distance_list = []\n",
        "\n",
        "for node in result_nodes:\n",
        "    text_list.append(node[0].page_content)\n",
        "    distance = node[1]\n",
        "    distance_list.append(distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs9QcQN97I_g"
      },
      "source": [
        "Display the search results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBHp5BiHKjIp"
      },
      "outputs": [],
      "source": [
        "print(f\"--> Total Documents Found: {len(result_nodes)}\")\n",
        "for node in result_nodes:\n",
        "  print(f\"--> {node[1]} | {node[0].page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBH1nheo7L7v"
      },
      "source": [
        "Prepare the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JBZHBpKMIV_"
      },
      "outputs": [],
      "source": [
        "messages = get_system_template(text_list, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkKG8oR17OKU"
      },
      "source": [
        "Call the model (the `invoke` function will check and populate the cache automatically):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7at9YHOWKG6"
      },
      "outputs": [],
      "source": [
        "timer_start = time.perf_counter()\n",
        "llm_response = llm.invoke(messages)\n",
        "timer_end = time.perf_counter()\n",
        "total_time = round(timer_end - timer_start, 4)\n",
        "print(f\"Total Time: {total_time}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URIIB4Hy7WrH"
      },
      "source": [
        "Print the LLM response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipERjc1aRcCC"
      },
      "outputs": [],
      "source": [
        "llm_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6oBBqwS7haw"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJakv8xO7i8Q"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_insight.png?raw=true\" alt=\"Callout - Check Redis Insight\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp36z0d-7hR7"
      },
      "source": [
        "A new document should appear on Redis, of type JSON. This is the cached response from the LLM.\n",
        "Notice that the key is made from a long this; this is a hash of the question.\n",
        "\n",
        "Because this is a basic cache, questions from the user will be hashed and compared against the key, which means that for this basic cache, questions must match exactly in order to be used.\n",
        "\n",
        "&nbsp;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gemlQkpZNLx-"
      },
      "source": [
        "#### Repeating the question to fetch results from the cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvHXlFXL8y5Z"
      },
      "source": [
        "When we ask exactly the same question as before, it should trigger a cache hit, meaning we will receive the answer from the Redis cache, much faster than calling the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qV4r9h-9NOcP"
      },
      "outputs": [],
      "source": [
        "timer_start = time.perf_counter()\n",
        "llm_response = llm.invoke(messages)\n",
        "timer_end = time.perf_counter()\n",
        "total_time = round(timer_end - timer_start, 4)\n",
        "print(f\"Total Time: {total_time}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btlVe7qM9G7T"
      },
      "source": [
        "Print the cached response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDGRpcHORY93"
      },
      "outputs": [],
      "source": [
        "llm_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4sszbPX9KXm"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4R86mhmNhuT"
      },
      "source": [
        "#### Asking the same question (worded differently) will cause a cache miss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZmHsS8t9MkC"
      },
      "source": [
        "If the question is not an exact match, it will cause a cache miss. This might be an issue with most of the RAG use cases, which is why we will be exploring Semantic Cache next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhRX_GoHNmuW"
      },
      "outputs": [],
      "source": [
        "# original query = \"How does Redis Insight make RDI simpler?\"\n",
        "query = \"What does Redis Insight do to make RDI simpler?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHQq4Shr_ZKf"
      },
      "source": [
        "Prepare the prompt with the new query (PS: we're skipping the vector search on purpose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCFhpjBgNwVa"
      },
      "outputs": [],
      "source": [
        "messages = get_system_template(text_list, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_JTMxC8_i9E"
      },
      "source": [
        "Call the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hogIqAOnNwQh"
      },
      "outputs": [],
      "source": [
        "timer_start = time.perf_counter()\n",
        "llm_response = llm.invoke(messages)\n",
        "timer_end = time.perf_counter()\n",
        "total_time = round(timer_end - timer_start, 4)\n",
        "print(f\"Total Time: {total_time}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9W1P_M6_mDp"
      },
      "source": [
        "Print the response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzO7OAVoNwJt"
      },
      "outputs": [],
      "source": [
        "llm_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPVB6DDX_1LG"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbNoHMtFN45X"
      },
      "source": [
        "## Part 7 - Leveraging Redis for Semantic Cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwR5FEo2AJjK"
      },
      "source": [
        "The Semantic Cache will generate vectors for each prompt, and store the response from the LLM. That way, new prompts are converted into vectors automatically and a semantic search is executed on Redis, looking for similar questions.\n",
        "\n",
        "It is possible to set the threshold for for semantic search; for this lab, we are using 20%. In your project, you can run multiple tests with different thresholds, to determined what works best for your use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFuDm3vwOCJg"
      },
      "outputs": [],
      "source": [
        "from langchain_redis import RedisSemanticCache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv75yqFrAqry"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_threshold.png?raw=true\" alt=\"Callout - Semantic Threshold\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA-yaqYtN-bR"
      },
      "outputs": [],
      "source": [
        "redis_cache = RedisSemanticCache(redis_url=REDIS_URL, embeddings=embeddings, distance_threshold=0.2)\n",
        "set_llm_cache(redis_cache)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv6nR-AFBEVZ"
      },
      "source": [
        "Since the Semantic Cache is new, it will be empty. We will ask the original question first, to generate the cache entry:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm3SLk2u-SWn"
      },
      "outputs": [],
      "source": [
        "query = \"How does Redis Insight make RDI simpler?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjeCV10VBMD8"
      },
      "source": [
        "Prepare the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3Pvy352-Rsb"
      },
      "outputs": [],
      "source": [
        "messages = get_system_template(text_list, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWKkgnnoBO-d"
      },
      "source": [
        "Invoke the model (it will cause a cache miss):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJGYVojmWUdB"
      },
      "outputs": [],
      "source": [
        "timer_start = time.perf_counter()\n",
        "llm_response = llm.invoke(messages)\n",
        "timer_end = time.perf_counter()\n",
        "total_time = round(timer_end - timer_start, 4)\n",
        "print(f\"Total Time: {total_time}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--0PfUm1BUa8"
      },
      "source": [
        "Print the response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heR894L4-lHd"
      },
      "outputs": [],
      "source": [
        "llm_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNVN4Q2MBYOQ"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoqlHw8DO0zL"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_insight.png?raw=true\" alt=\"Callout - Check Redis Insight\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoO1iZmVO1zW"
      },
      "source": [
        "A new Hash document will appear in Redis, with a key prefix of `llmcache`. This is the cached prompt, which includes the question and the answer. The `invoke` function will run a semantic search for these documents, to look for similar questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDfgdGQnO_ci"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3iZspjDQc_b"
      },
      "source": [
        "#### Ask a similar question to trigger a cache hit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1dac3x9QV-h"
      },
      "outputs": [],
      "source": [
        "query = \"What does Redis Insight do to make RDI simpler?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5WV9CfJBedI"
      },
      "source": [
        "Prepare the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6ekUthmQXVh"
      },
      "outputs": [],
      "source": [
        "messages = get_system_template(text_list, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1Syj2_WBgTC"
      },
      "source": [
        "Invoke the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNeUsew8QXSu"
      },
      "outputs": [],
      "source": [
        "timer_start = time.perf_counter()\n",
        "llm_response = llm.invoke(messages)\n",
        "timer_end = time.perf_counter()\n",
        "total_time = round(timer_end - timer_start, 4)\n",
        "print(f\"Total Time: {total_time}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e78ba7Y5Bi66"
      },
      "source": [
        "Print the response:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt_hlPsRQXQE"
      },
      "outputs": [],
      "source": [
        "llm_response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAVzG-35N0JI"
      },
      "source": [
        "&nbsp;\n",
        "\n",
        "\n",
        "&nbsp;\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G4eIa8hN2T9"
      },
      "source": [
        "# Congrats, this is the end of the lab!!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
