{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N09X5wKjBIsT"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denisabrantesredis/denisd-GenAI-Workshop/blob/main/Labs/02-LLM/02_Redis_Gemini_LLM.ipynb\" target=\"_newt\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tk5ymdLBTeq"
      },
      "source": [
        "<div style=\"display:flex;width=100%;\">\n",
        "<img src=\"https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120\" alt=\"Redis\" width=\"90\"/>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<img src=\"https://www.gstatic.com/devrel-devsite/prod/v0e0f589edd85502a40d78d7d0825db8ea5ef3b99ab4070381ee86977c9168730/cloud/images/cloud-logo.svg\" alt=\"Google Cloud\" width=\"140\"/>\n",
        "</div>\n",
        "\n",
        "# LLM Memory with Redis & Google Cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWGXHwYEOZoT"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/redis_gcp.png?raw=true\" alt=\"Redis and Google Cloud\" align=\"center\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8w5OAHNOlFf"
      },
      "source": [
        "In this exercise, we will create a conversational chat bot using Google Gemini. We will use Redis to score the conversation history to provide the model with the necessary context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENspmGoTR6tq"
      },
      "source": [
        "## Installing the Pre-Reqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFmBBQ0bBFym"
      },
      "outputs": [],
      "source": [
        "!pip install -q redis==5.0.8 >> /.tmp\n",
        "!pip install -q langchain==0.3.25 >> /.tmp\n",
        "!pip install -q langchain-core==0.3.59 >> /.tmp\n",
        "!pip install -q langchain-redis==0.2.0 >> /.tmp\n",
        "!pip install -q langchain-google-genai==2.1.4 >> /.tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installing Redis Stack Locally\n",
        "If you are not using Redis Cloud as a database, uncomment and run the code below to install Redis locally. Then set your connection to 127.0.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%sh\n",
        "# curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg \n",
        "# echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list \n",
        "# sudo apt-get update  > /dev/null 2>&1\n",
        "# sudo apt-get install redis-stack-server  > /dev/null 2>&1\n",
        "# redis-stack-server --daemonize yes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7_A4sOeDfqt"
      },
      "source": [
        "## Part 1 - Load and Configure the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ6RwklAct5K"
      },
      "source": [
        "In this lab, we will use the [Unstructured](https://docs.unstructured.io/open-source/core-functionality/partitioning#partition-html) API to load data from a web page, parse it and break into chunks.\n",
        "\n",
        "A web page can have multiple different types of content; this class will help us identify the type of content being collected from the page, so we can make sure we're only getting the text from the page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ko4lH56BFs3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import redis\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain_redis import RedisChatMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQkY7YrLkUZD"
      },
      "source": [
        "### Step 1: Setting Up Connection String"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfApethtkegP"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_secrets.png?raw=true\" alt=\"Callout - Use Google Colab secrets instead\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTHiBGT_GYac"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "except:\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = \"<insert API key here>\"\n",
        "\n",
        "try:\n",
        "  REDIS_HOST = userdata.get('REDIS_HOST')\n",
        "except:\n",
        "  REDIS_HOST=\"127.0.0.1\"\n",
        "\n",
        "try:\n",
        "  REDIS_PORT = userdata.get('REDIS_PORT')\n",
        "except:\n",
        "  REDIS_PORT=6379\n",
        "\n",
        "try:\n",
        "  REDIS_PASSWORD = userdata.get('REDIS_PASSWORD')\n",
        "except:\n",
        "  REDIS_PASSWORD=\"\"\n",
        "\n",
        "REDIS_URL = f\"redis://default:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Testing the Connection to Redis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_connection.png?raw=true\" alt=\"Callout - Make sure connection works\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "r = redis.from_url(REDIS_URL)\n",
        "\n",
        "if r.ping():\n",
        "    print(\"Connection successful!\")\n",
        "else:\n",
        "    print(\"Connection issue!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq-_RvOKmVr7"
      },
      "source": [
        "### Step 2 - Load the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYRtnOltmyC4"
      },
      "source": [
        "A session identifier is needed to separate conversations per user (or session). We'll use the same Langchain package from the previous labs to orchestrate the conversation with the model. For this use case, we will use `RedisChatMessageHistory`to keep track of the conversation history between the user and the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_session2.png?raw=true\" alt=\"Callout - Session Identifier\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0H9huxKHI4q"
      },
      "outputs": [],
      "source": [
        "user_session = \"default\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    top_k=64,\n",
        "    max_output_tokens=8192\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this lab, we will keep the prompt very simple, and ensure that the conversation history is included with every interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a conversational chain\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "chain = prompt | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using Redis to store conversation history is very simple; the storing and retrieval of conversation history is automatically handled by Langchain. For this lab, we're setting a Time-To-Live (TTL) of 3,600 seconds, or 1 hour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to get or create a RedisChatMessageHistory instance\n",
        "def get_redis_history(session_id: str):\n",
        "    return RedisChatMessageHistory(session_id, redis_url=REDIS_URL, ttl=3600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a runnable with message history\n",
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_redis_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_llmfunction.png?raw=true\" alt=\"Callout - LLM Function\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(input_text, user_session):\n",
        "    response = chain_with_history.invoke({\"input\": input_text}, config={\"configurable\": {\"session_id\": user_session}})\n",
        "    return response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2 - Talking to the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Large Language Models do not keep track of the conversations they are having with users; each question is received (and answered) in a completely isolated way, with no context of previous interactions. However, behavior like this would make for very poor interactions, as users need to contextualize what they are trying to achieve, often across multiple interactions.\n",
        "\n",
        "In order to provide a better user experience, client UIs (like chatbots) often keep track of the previous interactions between users and models, and send this context to the model with every new question. That way, the model can 'read' the previous interactions and contextualize its response in a way that emulate a natural conversation between people."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Y3Fy_JnSm5"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_1q.png?raw=true\" alt=\"Callout - First Question to the Model\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLUdjv5KHOpJ"
      },
      "outputs": [],
      "source": [
        "chat_input = \"What is the capital of Canada?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_j8oVZIzgvv"
      },
      "source": [
        "Send the question to the model and print the response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g-MMhPNze4Z"
      },
      "outputs": [],
      "source": [
        "response = generate_response(chat_input, user_session)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Markdown(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jHy0dVgsAAg"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDX0QOboqped"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_insight.png?raw=true\" alt=\"Callout - Check Redis Insight\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxBu0RKZqu4t"
      },
      "source": [
        "Open Redis Insight and you will find 2 new documents there, one for the question and one for the answeer.\n",
        "\n",
        "Notice the additional metadata, such as session identifier, timestamp, etc.\n",
        "\n",
        "&nbsp;\n",
        "\n",
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS3kewVQvR8y"
      },
      "source": [
        "### Asking a follow-up session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4WSqBSVvZCV"
      },
      "source": [
        "The Langchain integration greatly simplifies the process of running a semantic search. A single function call is enough. Notice how we do not need to generate a vector for our question manually; this is handled automatically by the function, based on the embedding model we've selected before.\n",
        "\n",
        "For more details on the different ways to run vector searches, check the [Langchain documentation page](https://python.langchain.com/docs/integrations/vectorstores/redis/#query-vector-store).\n",
        "\n",
        "&nbsp;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_2q.png?raw=true\" alt=\"Callout - Follow-up Question to the Model\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-n9ER0lHi12"
      },
      "outputs": [],
      "source": [
        "chat_input = \"How come it's not Toronto?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FKaU45iwULg"
      },
      "source": [
        "### Sending the follow-up question to the model\n",
        "\n",
        "Without the conversation history, the model would be confused with this new question, seeing as it would be missing the context from the previous interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wepEg9G0umFH"
      },
      "outputs": [],
      "source": [
        "response = generate_response(chat_input, user_session)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Markdown(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_insight.png?raw=true\" alt=\"Callout - Check Redis Insight\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's an interesting exercise: open Redis Insight and delete all documents. This will cause the model to get confused when we ask another follow-up question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_3q.png?raw=true\" alt=\"Callout - Another Follow-up Question to the Model\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_input = \"What other cities would be good candidates?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = generate_response(chat_input, user_session)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Markdown(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZUaPXegyFyx"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UALXxSxHHugw"
      },
      "source": [
        "## Part 3: Behind the Scenes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC3h0h55yKKJ"
      },
      "source": [
        "In this lab, we will use the Gemini Pro 1.5 model from Google to generate a response to the user, based on the documents retrieved from Redis. The GCP API Key that we set before is required to allow access to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3UgzWI31G6P"
      },
      "source": [
        "#### Looking at the Message History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAMSsoWgHw2u"
      },
      "outputs": [],
      "source": [
        "get_redis_history(user_session).messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Querying Redis Directly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the conversation history is stored in Redis as documents, we can access them directly for other use cases (for instance, a call center that wants to review the interaction between user and model, or a data sciences team gathering data to fine-tune a conversational model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run a Search for documents that contain the word 'city'. Because of [stemming](https://redis.io/docs/latest/develop/interact/search-and-query/advanced-concepts/stemming/), the response will include question and answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_search(keyword):\n",
        "  try:\n",
        "    result = r.ft(\"idx:chat_history\").search(f'@content:{keyword}').docs\n",
        "  except Exception as e:\n",
        "    result = f\"Error: {e}\"\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = run_search(\"city\")\n",
        "for result in results:\n",
        "  print(result.json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Render conversation history in HTML format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = []\n",
        "for result in results:\n",
        "  jsondoc = json.loads(result.json)\n",
        "  msg_from = jsondoc[\"type\"]\n",
        "  msg_content = jsondoc[\"data\"][\"content\"]\n",
        "  ts = jsondoc['timestamp']\n",
        "  msg_ts = datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
        "  \n",
        "  message = {\n",
        "      \"from\" : msg_from,\n",
        "      \"timestamp\" : msg_ts,\n",
        "      \"content\" : msg_content\n",
        "  }\n",
        "\n",
        "  messages.append(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "html_start = \"\"\"\n",
        "<html><body>\n",
        "<table style='border:1px solid gray;padding:2px;float:left;'>\n",
        "    <tr>\n",
        "      <th style='text-align:center;'>From</th>\n",
        "      <th style='text-align:center;'>Timestamp</th>\n",
        "      <th style='text-align:center;'>Content</th>\n",
        "  </tr>\n",
        "\"\"\"\n",
        "html_body = \"\"\n",
        "for message in messages:\n",
        "  html_body += f\"\"\"\n",
        "    <tr style='border:1px solid gray;padding:2px;'>\n",
        "        <td style='text-align:center;width:10%;'>{message['from']}</td>\n",
        "        <td style='text-align:center;width:20%;'>{message['timestamp']}</td>\n",
        "        <td style='text-align:left;width:70%;'>{message['content']}</td>\n",
        "    </tr>\n",
        "  \"\"\"\n",
        "html_end = \"\"\"\n",
        "</table></body></html>\n",
        "\"\"\"\n",
        "html_full = html_start + html_body + html_end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HTML(html_full)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAVzG-35N0JI"
      },
      "source": [
        "&nbsp;\n",
        "\n",
        "\n",
        "&nbsp;\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Important\n",
        "\n",
        "Redis can be used as Vector Database, Semantic Cache and LLM Memory in the same use case; This allows for much faster interactions and an overall better user experience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/diagram_redis_cache_llm.png?raw=true\" alt=\"Diagram - Redis as Semantic Cache and LLM Memory\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G4eIa8hN2T9"
      },
      "source": [
        "# Congrats, this is the end of the lab!!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
