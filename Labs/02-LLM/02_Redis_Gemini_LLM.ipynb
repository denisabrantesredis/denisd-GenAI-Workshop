{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N09X5wKjBIsT"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denisabrantesredis/denisd-GenAI-Workshop/blob/main/Labs/02-LLM/02_Redis_Gemini_LLM.ipynb\" target=\"_newt\">\n",
        "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tk5ymdLBTeq"
      },
      "source": [
        "<div style=\"display:flex;width=100%;\">\n",
        "<img src=\"https://redis.io/wp-content/uploads/2024/04/Logotype.svg?auto=webp&quality=85,75&width=120\" alt=\"Redis\" width=\"90\"/>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<img src=\"https://www.gstatic.com/devrel-devsite/prod/v0e0f589edd85502a40d78d7d0825db8ea5ef3b99ab4070381ee86977c9168730/cloud/images/cloud-logo.svg\" alt=\"Google Cloud\" width=\"140\"/>\n",
        "</div>\n",
        "\n",
        "# LLM Memory with Redis & Google Cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWGXHwYEOZoT"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/redis_gcp.png?raw=true\" alt=\"Redis and Google Cloud\" align=\"center\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8w5OAHNOlFf"
      },
      "source": [
        "[Try a similar app with an always-on demo](https://antonum-redis-vss-streamlit-streamlit-app-p4z5th.streamlit.app/)\n",
        "\n",
        "In this notebook, we will build a RAG use case using data from a web page. Redis will be used as the Vector Database and Cache for our use case, while Google Gemini is the LLM that will help generate the answers to the user's questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENspmGoTR6tq"
      },
      "source": [
        "## Installing the Pre-Reqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFmBBQ0bBFym"
      },
      "outputs": [],
      "source": [
        "# !pip install -q sentence-transformers==3.0.1 >> /.tmp\n",
        "!pip install -q redis==5.0.8 >> /.tmp\n",
        "!pip install -q langchain==0.2.16 >> /.tmp\n",
        "!pip install -q langchain-core==0.3.6 >> /.tmp\n",
        "!pip install -q langchain-redis==0.0.4 >> /.tmp\n",
        "!pip install -q langchain-google-genai==2.0.0 >> /.tmp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # patch an issue with RedisVL\n",
        "# !wget https://github.com/denisabrantesredis/denisd-GenAI-Workshop/raw/refs/heads/main/_assets/files/semantic.py\n",
        "# !rm /usr/local/lib/python3.10/dist-packages/redisvl/extensions/llmcache/semantic.py\n",
        "# !cp semantic.py /usr/local/lib/python3.10/dist-packages/redisvl/extensions/llmcache/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7_A4sOeDfqt"
      },
      "source": [
        "## Part 1 - Load and Configure the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ6RwklAct5K"
      },
      "source": [
        "In this lab, we will use the [Unstructured](https://docs.unstructured.io/open-source/core-functionality/partitioning#partition-html) API to load data from a web page, parse it and break into chunks.\n",
        "\n",
        "A web page can have multiple different types of content; this class will help us identify the type of content being collected from the page, so we can make sure we're only getting the text from the page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ko4lH56BFs3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain_redis import RedisChatMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQkY7YrLkUZD"
      },
      "source": [
        "### Step 1: Setting Up Connection String"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfApethtkegP"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_secrets.png?raw=true\" alt=\"Callout - Use Google Colab secrets instead\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTHiBGT_GYac"
      },
      "outputs": [],
      "source": [
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    if userdata.get('GOOGLE_API_KEY'):\n",
        "      os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "    else:\n",
        "      os.environ[\"GOOGLE_API_KEY\"] = \"<insert API key here>\"\n",
        "\n",
        "if userdata.get('REDIS_HOST'):\n",
        "  REDIS_HOST = userdata.get('REDIS_HOST')\n",
        "else:\n",
        "  REDIS_HOST=\"127.0.0.1\"\n",
        "\n",
        "if userdata.get('REDIS_PORT'):\n",
        "  REDIS_PORT = userdata.get('REDIS_PORT')\n",
        "else:\n",
        "  REDIS_PORT=12000\n",
        "\n",
        "if userdata.get('REDIS_PASSWORD'):\n",
        "  REDIS_PASSWORD = userdata.get('REDIS_PASSWORD')\n",
        "else:\n",
        "  REDIS_PASSWORD=\"password\"\n",
        "\n",
        "REDIS_URL = f\"redis://default:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq-_RvOKmVr7"
      },
      "source": [
        "### Step 2 - Load the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYRtnOltmyC4"
      },
      "source": [
        "In this step, we prepare a list of JSON objects contanining the data from our chunks. Here is where we can map the metadata fields we want to store in Redis to be used in hybrid searches. Notice how we are not generating the vectors manually as part of the step; this is fully automated by the Langchain package, based on the embedding model we've selected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0H9huxKHI4q"
      },
      "outputs": [],
      "source": [
        "user_session = \"default\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0.5,\n",
        "    top_p=0.95,\n",
        "    top_k=64,\n",
        "    max_output_tokens=8192\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a conversational chain\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to get or create a RedisChatMessageHistory instance\n",
        "def get_redis_history(session_id: str):\n",
        "    return RedisChatMessageHistory(session_id, redis_url=REDIS_URL, ttl=3600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a runnable with message history\n",
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_redis_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(input_text, user_session):\n",
        "    response = chain_with_history.invoke({\"input\": input_text}, config={\"configurable\": {\"session_id\": user_session}})\n",
        "    return response.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Y3Fy_JnSm5"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_save.png?raw=true\" alt=\"Callout - Saving to Redis\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLUdjv5KHOpJ"
      },
      "outputs": [],
      "source": [
        "chat_input = \"What is the capital of Canada?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_j8oVZIzgvv"
      },
      "source": [
        "List the IDs of all documents saved to Redis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g-MMhPNze4Z"
      },
      "outputs": [],
      "source": [
        "response = generate_response(chat_input, user_session)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Markdown(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jHy0dVgsAAg"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDX0QOboqped"
      },
      "source": [
        "<img src=\"https://github.com/denisabrantesredis/denisd-GenAI-Workshop/blob/main/_assets/images/callout_insight.png?raw=true\" alt=\"Callout - Check Redis Insight\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxBu0RKZqu4t"
      },
      "source": [
        "Open Redis Insight and confirm that all documents were generated. Notice how each document contains the vector that was automatically generated by the Langchain package. You may also notice that the vectors are not presented as a list; this is due to the fact that they are stored as binary strings, which is more efficient for retrieval and storage.\n",
        "\n",
        "You can also go to the **Workbench** and get a list of indexes using the command:\n",
        "\n",
        "```\n",
        "FT._list\n",
        "```\n",
        "\n",
        "Finally, you can get more details about the index that was automatically generated by Langchain with this command:\n",
        "```\n",
        "FT.info \"idx:web\"\n",
        "```\n",
        "&nbsp;\n",
        "\n",
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS3kewVQvR8y"
      },
      "source": [
        "### Running a Semantic Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4WSqBSVvZCV"
      },
      "source": [
        "The Langchain integration greatly simplifies the process of running a semantic search. A single function call is enough. Notice how we do not need to generate a vector for our question manually; this is handled automatically by the function, based on the embedding model we've selected before.\n",
        "\n",
        "For more details on the different ways to run vector searches, check the [Langchain documentation page](https://python.langchain.com/docs/integrations/vectorstores/redis/#query-vector-store).\n",
        "\n",
        "&nbsp;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-n9ER0lHi12"
      },
      "outputs": [],
      "source": [
        "chat_input = \"How come it's not Toronto?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FKaU45iwULg"
      },
      "source": [
        "### Visualizing the search results with the score for each result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wepEg9G0umFH"
      },
      "outputs": [],
      "source": [
        "response = generate_response(chat_input, user_session)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Markdown(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chat_input = \"What other cities would be good candidates?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = generate_response(chat_input, user_session)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Markdown(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZUaPXegyFyx"
      },
      "source": [
        "&nbsp;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UALXxSxHHugw"
      },
      "source": [
        "## Part 5: Using a LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC3h0h55yKKJ"
      },
      "source": [
        "In this lab, we will use the Gemini Pro 1.5 model from Google to generate a response to the user, based on the documents retrieved from Redis. The GCP API Key that we set before is required to allow access to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3UgzWI31G6P"
      },
      "source": [
        "### Step 1: Load the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAMSsoWgHw2u"
      },
      "outputs": [],
      "source": [
        "get_redis_history(user_session).messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDNxn4XlIc6T"
      },
      "outputs": [],
      "source": [
        "import redis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "r = redis.Redis(host=redis_host, port=redis_port, username=redis_user, password=redis_pw, decode_responses=True)\n",
        "\n",
        "if r.ping():\n",
        "    print(\"Connection successful!\")\n",
        "else:\n",
        "    print(\"Connection issue!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAVzG-35N0JI"
      },
      "source": [
        "&nbsp;\n",
        "\n",
        "\n",
        "&nbsp;\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G4eIa8hN2T9"
      },
      "source": [
        "# Congrats, this is the end of the lab!!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
